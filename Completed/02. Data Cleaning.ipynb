{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Let's load the some data for TSLA. Unfortunately, this data is not quite as *clean* as our NVDA data, so we'll need to do some data wrangling. The file we're looking to load is `TSLA_2015_2024.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New imports for a new notebook!\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data, setting the index, and having a look\n",
    "df = pd.read_csv(\"data/TSLA_2015_2024.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see what we mean by messy? How many issues can you spot?\n",
    "\n",
    "- Dates out of order\n",
    "- Duplicate rows\n",
    "- Missing values\n",
    "\n",
    "## Ordering and Duplicates\n",
    "\n",
    "First let's start with sorting the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for ascending order in the index\n",
    "df.index.is_monotonic_increasing\n",
    "\n",
    "# Sorting the index in place\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Checking for ascending order in the index\n",
    "df.index.is_monotonic_increasing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's focus on duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting duplicates\n",
    "df.duplicated().sum()\n",
    "\n",
    "# Getting rid of duplicates\n",
    "df.drop_duplicates()\n",
    "\n",
    "# But make sure we update the variable to save our work!\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Counting duplicates\n",
    "df.duplicated().sum()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip: Method Chaining\n",
    "\n",
    "**Method chaining** is a popular feature of pandas. It allows us to *chain* together several operations in a single line of code. For example, we can set the index, sort the data frame and drop any duplicates all at once. Notice we don't use `inplace` but rather re-assign to the original `df` variable.\n",
    "\n",
    "```python\n",
    "df = df.set_index(\"Date\").sort_index().drop_duplicates()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not a Number (NaN)\n",
    "\n",
    "### Exercise: Some Null Chain\n",
    "\n",
    "Let's look at the missing or `NaN` values next. Previously, we saw that `info()` gave us some insight into how many missing values we had, but we can also use `isnull()`.\n",
    "\n",
    "Can you chain `isnull()` with `sum()` to get a single value stating the total number of missing values in the data frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out which rows have missing data using `isnull()`, `any()` along rows and some smart *masking*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've identified our missing values, the big question is how to handle them. There are many approaches to this that will vary depending on the data and the further analysis you plan to carry out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just drop any row that contains NaN in any column\n",
    "df.dropna()\n",
    "\n",
    "# Drop a row which contains NaN in a specific column\n",
    "df.dropna(subset=\"Close\")\n",
    "\n",
    "# We can fill in NaNs with the average of a column\n",
    "df['Volume'].fillna(df['Volume'].mean())\n",
    "\n",
    "# We can interpolate (point on a line connecting the value of the days before and after)\n",
    "df[\"Close\"].interpolate(method=\"linear\")\n",
    "\n",
    "# We can forward fill, and use the value from the day before\n",
    "df[\"Close\"].ffill()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cleaning up\n",
    "\n",
    "Notice how above we didn't actually update the `df` variable, so our DataFrame is still full of missing values. Fix all missing values applying the following rules:\n",
    "- Fill missing Close by linear interpolation\n",
    "- Fill missing Volume with the value from the day before\n",
    "- Fill missing Open with the median Open\n",
    "- Fill missing High with the Close or Open, whichever is higher\n",
    "- Fill missing Low with a value 3% lower than the High\n",
    "\n",
    "\n",
    "Your DataFrame `df` should have no missing values when done. Use `info()` to confirm.\n",
    "\n",
    "**NOTE:** When changing values in a data frame, it is recommended to avoid using `inplace`, and instead re-assign the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE GOES HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced: Data Types\n",
    "\n",
    "You may have noticed that the **Volume** column in the 2021 data frame is a `float64` instead of the `int64` *dtype* we had in the 2020 data frame. Missing values (NaN) are represented as a special case of floating point number, so all the values in **Volume** were automatically *upcast* to floats.\n",
    "\n",
    "Ideally our columns should be of the *dtype* that most accurately represents them. This will improve performance when working with large data frames. Now that we've resolved our missing numbers, we can *cast* our trading volumes as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Volume\"] = df[\"Volume\"].astype(\"int64\")\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "Now that we've cleaned our data, let's save it, by writing it to a new .CSV file. We can use pandas' `to_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"TSLA_10_clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
